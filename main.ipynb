{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 Collection (11/04/2023 - 31/05/2023)\n",
    "\n",
    "During phase 1 collection, 2 NDIR CO2 sensors (model: Sensiron SCD30) were used to sense the CO2 levels within the glass container. Both sensor modules are equipped with temperature and humidity sensors that allows them to quantify exactly how hot and humid the inside of the container is.\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "The data collected from this has several observed patterns:\n",
    "\n",
    "\\<We also want to add data visualisations here about these key observed patterns\\>\n",
    "\n",
    "1. **Daily:** Temperatures will rise accordingly based on how sunny that day is. The highest temperature reached on any given day is ___. The temperature required for decomposition of PolyTerra PLA is stated by the manufacturer to be ___. \n",
    "2. **Daily:** Humidity will be lowered in reverse proportion to the temperature, as the water ___ (does the water condensate off?). \n",
    "3. **Overall Trend:** There was no observed increase in CO2 throughout the experimentation period\n",
    "\n",
    "Weight analysis: \\<To be done\\>\n",
    "\n",
    "\\<Add photos here of the probably decomposed plastics?\\>\n",
    "\n",
    "As such, we conclude that during phase 1, \\<conclusion here\\> PLA decomposition did/did not occur within the glass envelope."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 1: ingestion of data\n",
    "\n",
    "This process phase ingests and cleans multiple CSV files as data into a pandas array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ingest data\n",
    "co2_directory = 'CO2'\n",
    "li_co2 = []\n",
    "\n",
    "for filename in os.listdir(co2_directory):\n",
    "    f = os.path.join(co2_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and filename[-3:] == 'csv':\n",
    "        li_co2.append(pd.read_csv(f, parse_dates=[3], encoding='utf-8'))\n",
    "\n",
    "frame_co2 = pd.concat(li_co2, axis=0, ignore_index=True)\n",
    "\n",
    "hum_directory = 'Humidity'\n",
    "li_hum = []\n",
    "\n",
    "for filename in os.listdir(hum_directory):\n",
    "    f = os.path.join(hum_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and filename[-3:] == 'csv':\n",
    "        li_hum.append(pd.read_csv(f, parse_dates=[3], encoding='utf-8'))\n",
    "\n",
    "frame_hum = pd.concat(li_hum, axis=0, ignore_index=True)\n",
    "\n",
    "temp_directory = 'Temperature'\n",
    "li_temp = []\n",
    "\n",
    "for filename in os.listdir(temp_directory):\n",
    "    f = os.path.join(temp_directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f) and filename[-3:] == 'csv':\n",
    "        li_temp.append(pd.read_csv(f, parse_dates=[3], encoding='utf-8'))\n",
    "\n",
    "frame_temp = pd.concat(li_temp, axis=0, ignore_index=True)\n",
    "\n",
    "# clean data (strip metadata)\n",
    "frame_co2 = frame_co2.drop(columns=['id', 'feed_id', 'lat', 'lon', 'ele'])\n",
    "frame_hum = frame_hum.drop(columns=['id', 'feed_id', 'lat', 'lon', 'ele'])\n",
    "frame_temp = frame_temp.drop(columns=['id', 'feed_id', 'lat', 'lon', 'ele'])\n",
    "\n",
    "# localize the timezones\n",
    "frame_co2['created_at'] = frame_co2['created_at'].dt.tz_convert('Asia/Singapore')\n",
    "frame_hum['created_at'] = frame_hum['created_at'].dt.tz_convert('Asia/Singapore')\n",
    "frame_temp['created_at'] = frame_temp['created_at'].dt.tz_convert('Asia/Singapore')\n",
    "\n",
    "# make the datetime the index and dedupe the overlapping times\n",
    "frame_co2 = frame_co2.set_index('created_at').drop_duplicates()\n",
    "frame_hum = frame_hum.set_index('created_at').drop_duplicates()\n",
    "frame_temp = frame_temp.set_index('created_at').drop_duplicates()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 2: datetime processing\n",
    "\n",
    "The dates and times have to be processed to be normalized, 5 minute intervals in order to facilitate data correlation between the 3 types of data. This resultant data can be pickled and cached on disk for rapid re-computation when tweaking the visualization algos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all into one big dataframe\n",
    "frame_co2 = frame_co2.rename(columns = {'value':'co2'})\n",
    "frame_temp = frame_temp.rename(columns = {'value':'temp'})\n",
    "frame_hum = frame_hum.rename(columns = {'value':'hum'})\n",
    "\n",
    "frame_final = pd.merge(frame_co2, frame_hum, how='outer', on='created_at')\n",
    "frame_final = pd.merge(frame_final, frame_temp, how='outer', on='created_at')\n",
    "\n",
    "frame_final.sort_index().to_csv('debug.csv') # this is just for debug to see what the data looks like rn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cornmen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
